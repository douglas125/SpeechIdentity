{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f15cdde",
   "metadata": {},
   "source": [
    "# Train speech identity verification model\n",
    "\n",
    "Having prepared the tfrecords file, we are ready to train and evaluate some models.\n",
    "\n",
    "We use triplet loss, described at https://www.tensorflow.org/addons/tutorials/losses_triplet at the time this was written.\n",
    "\n",
    "Note that triplet loss requires similar and negative examples to be present in the batch, ie, audios from the same person and from someone different. We'd like to have the batches as large as we can to increase this probability if we're shuffling samples. Another option (not covered here) would be to force the existence of valid batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47543494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import layers as L\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from create_audio_tfrecords import AudioTarReader, PersonIdAudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8117459e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = [x for x in os.listdir('data') if x.endswith('train.tfrecords.gzip')]\n",
    "train_files = [os.path.join('data', x) for x in train_files]\n",
    "\n",
    "# check if tfrecords file is OK\n",
    "# notice GZIP compression + the deserialization function map\n",
    "tfrecords_audio_dataset = tf.data.TFRecordDataset(\n",
    "    train_files, compression_type='GZIP',\n",
    "    num_parallel_reads=4\n",
    ").map(PersonIdAudio.deserialize_from_tfrecords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b356eed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of records\n",
    "n_train_samples = sum(1 for _ in tfrecords_audio_dataset)\n",
    "print(n_train_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb457c42",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affc3b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mel_bins = 80\n",
    "\n",
    "def normalized_mel_spectrogram(x, sr=48000):\n",
    "    spec_stride = 256\n",
    "    spec_len = 1024\n",
    "\n",
    "    spectrogram = tfio.audio.spectrogram(\n",
    "        x, nfft=spec_len, window=spec_len, stride=spec_stride\n",
    "    )\n",
    "\n",
    "    num_spectrogram_bins = spec_len // 2 + 1  # spectrogram.shape[-1]\n",
    "    lower_edge_hertz, upper_edge_hertz, num_mel_bins = 80.0, 10000.0, n_mel_bins\n",
    "    linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\n",
    "      num_mel_bins, num_spectrogram_bins, sr, lower_edge_hertz,\n",
    "      upper_edge_hertz)\n",
    "    mel_spectrograms = tf.tensordot(\n",
    "      spectrogram, linear_to_mel_weight_matrix, 1)\n",
    "    mel_spectrograms.set_shape(spectrogram.shape[:-1].concatenate(\n",
    "      linear_to_mel_weight_matrix.shape[-1:]))\n",
    "\n",
    "    # Compute a stabilized log to get log-magnitude mel-scale spectrograms.\n",
    "    log_mel_spectrograms = tf.math.log(mel_spectrograms + 1e-6)\n",
    "    avg = tf.math.reduce_mean(log_mel_spectrograms)\n",
    "    std = tf.math.reduce_std(log_mel_spectrograms)\n",
    "\n",
    "    return (log_mel_spectrograms - avg) / std\n",
    "\n",
    "\n",
    "def BaseSpeechEmbeddingModel(inputLength=None, rnn_func=L.LSTM, rnn_units=64):\n",
    "    # input is the first channel of the decoded mp3, ie, \n",
    "    # tfio.audio.decode_mp3(data)[:, 0]\n",
    "\n",
    "    # inp = L.Input((inputLength,), name='input')\n",
    "    # mel_spec = L.Lambda(lambda z: normalized_mel_spectrogram(z), name='normalized_spectrogram')(inp)\n",
    "\n",
    "    # receive normalized mel spectrogram as input instead\n",
    "    inp = L.Input((inputLength, n_mel_bins), name='input')\n",
    "    mel_spec = inp\n",
    "\n",
    "    # normalize the spectrogram\n",
    "    # mel_spec = L.BatchNormalization()(mel_spec)\n",
    "    # mel_spec = L.LayerNormalization()(mel_spec)\n",
    "\n",
    "    x = L.Bidirectional(\n",
    "        rnn_func(rnn_units, return_sequences=True)\n",
    "    )(mel_spec)  # [b_s, seq_len, vec_dim]\n",
    "    x = L.Bidirectional(\n",
    "        rnn_func(rnn_units, return_sequences=False)\n",
    "    )(x)  # [b_s, seq_len, vec_dim]\n",
    "\n",
    "    x = L.Dense(rnn_units, activation=None)(x)  # No activation on final dense layer\n",
    "    # L2 normalize embeddings\n",
    "    # note: L2 returns normalized, norm\n",
    "    x = L.Lambda(lambda z: tf.math.l2_normalize(z, axis=1), name='output')(x)\n",
    "    \n",
    "    output = x\n",
    "\n",
    "    model = Model(inputs=[inp], outputs=[output])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7073a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = BaseSpeechEmbeddingModel()\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc01ffd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f722b03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples = [x for x in tfrecords_audio_dataset.take(2)]\n",
    "# samples[0][0].shape, samples[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ae0659",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03ee012a",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Note: we need to `shuffle -> repeat -> batch` in this order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9f4af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 96\n",
    "\n",
    "return_mel_spec = True\n",
    "def mp3_decode_fn(audio_bytes, audio_class):\n",
    "    # check if limiting output size helps\n",
    "    # return tfio.audio.decode_mp3(audio_bytes)[:, 0], audio_class\n",
    "    audio_data = tfio.audio.decode_mp3(audio_bytes)[:, 0]\n",
    "    # audio_data = tfio.audio.decode_mp3(audio_bytes)[0:48000 * 4, 0]\n",
    "    if return_mel_spec:\n",
    "        audio_data = normalized_mel_spectrogram(audio_data)\n",
    "    return audio_data, audio_class\n",
    "\n",
    "train_set = tfrecords_audio_dataset.map(\n",
    "        # Reduce memory usage\n",
    "        mp3_decode_fn,\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    ).shuffle(\n",
    "        10 * batch_size,\n",
    "        reshuffle_each_iteration=True\n",
    "    ).repeat(\n",
    "    ).padded_batch(  # Vectorize your mapped function\n",
    "        batch_size,  # batch size\n",
    "        padded_shapes=([None, None], []),\n",
    "        drop_remainder=True\n",
    "    ).prefetch(  # Overlap producer and consumer works\n",
    "        tf.data.AUTOTUNE\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65baf8a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd486cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples = [x for x in train_set.take(2)]\n",
    "# samples[0][0].shape, samples[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97b0f35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa8b946",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    loss=tfa.losses.TripletSemiHardLoss()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3758c248",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = m.fit(\n",
    "    train_set,\n",
    "    steps_per_epoch = n_train_samples // batch_size,\n",
    "    epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81ca560",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ef3c583",
   "metadata": {},
   "source": [
    "## Check similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9237c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from create_audio_tfrecords import AudioTarReader\n",
    "audio_tarfile = 'data/cv-corpus-7.0-2021-07-21-pt.tar.gz'\n",
    "\n",
    "atr = AudioTarReader(audio_tarfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5b76b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_audio_content = atr.retrieve_per_user_data('dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b51684c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "def get_embedding(data, model):\n",
    "    preds = []\n",
    "    for x in tqdm(data):\n",
    "        audio_data = tfio.audio.decode_mp3(x)[:, 0]\n",
    "        audio_data = normalized_mel_spectrogram(audio_data)\n",
    "        cur_pred = model.predict(\n",
    "            tf.expand_dims(audio_data, axis=0)\n",
    "        )[0]\n",
    "        preds.append(cur_pred)\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081ce094",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_content_with_repeats = [x for x in val_audio_content if len(val_audio_content[x]) > 1]\n",
    "print([len(val_audio_content[x]) for x in audio_content_with_repeats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce797ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val_audio_content[audio_content_with_repeats[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3866560",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keys = audio_content_with_repeats\n",
    "samples1 = val_audio_content[all_keys[4]]\n",
    "samples2 = val_audio_content[all_keys[18]]\n",
    "preds1 = get_embedding(samples1, m)\n",
    "preds2 = get_embedding(samples2, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9331a270",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_dists(list1, list2):\n",
    "    ans = []\n",
    "    for x in tqdm(list1):\n",
    "        for y in list2:\n",
    "            dist = np.linalg.norm(x-y)\n",
    "            ans.append(dist)\n",
    "    return ans\n",
    "\n",
    "local_dists1 = get_dists(preds1, preds1)\n",
    "local_dists2 = get_dists(preds2, preds2)\n",
    "cross_dists = get_dists(preds1, preds2)\n",
    "\n",
    "np.mean(local_dists1), np.mean(local_dists2), np.mean(cross_dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84984f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
