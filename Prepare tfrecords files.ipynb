{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54e2a257",
   "metadata": {
    "id": "54e2a257"
   },
   "source": [
    "# Prepare tfrecords files\n",
    "\n",
    "Perform sanity check using low amount of data\n",
    "\n",
    "- Prepare dataset\n",
    "- Train model\n",
    "\n",
    "Full scale training\n",
    "\n",
    "- What can fit memory?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jZjcRSdPmW0k",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jZjcRSdPmW0k",
    "outputId": "a16fe39f-6dee-48c4-a01c-4859d499f25d"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('data', exist_ok=True)\n",
    "# if not os.path.isfile('data/cv-corpus-7.0-2021-07-21-it.tar.gz'):\n",
    "#     !wget -O \"data/cv-corpus-7.0-2021-07-21-it.tar.gz\" \"https://mozilla-common-voice-datasets.s3.dualstack.us-west-2.amazonaws.com/cv-corpus-7.0-2021-07-21/cv-corpus-7.0-2021-07-21-it.tar.gz?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=ASIAQ3GQRTO3CHAF2LUR%2F20210801%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20210801T075430Z&X-Amz-Expires=43200&X-Amz-Security-Token=FwoGZXIvYXdzENn%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaDGTMWIPnTt46zTwUTCKSBEzVQleBRJYCDi9aNljFC0TrxdJ5O%2BtQ%2FY0wwhg8b7X4mD6Tu%2BIQ6yAtcJ20qW5vYW4nv0PvpNrq7Ne%2FoZ0RWT9j1wedHxleh2g3JEP4HE8FUKmpVzb5HiaKGmBYRn41nnM0Czk3WHD7KeHHhtQj5rMTxbmHQUTw7gvad7ieRy%2FF4WbOzX%2FPx78dt4Zq2%2BTxl%2Fc4SOhlM9n3SKWc0foqKuDzytDFf1%2FQd45BMUWCiPOd2fyf0l751fgygj7syaEnegchts96%2FZZ0ilaXYuu9jjcO7gJCMB32r6rndMP5g98RbV5ScPe5Ey7TvAgeKZXFuF5LHIj7TraBr0Z6WqX02Salo9c%2Fu5b%2B%2FurhR5Q6%2B5hDbvg9abIrAzpom5egeOJSDFTYzsQHOdboXgs7Ciop7YktBjHXMTPi7ck22%2F4OYI4lqdwLICn%2BHE%2B79%2FcDrTYQ%2BOSLYZonbIc2u9Q2iHwjWr4i9Z%2BGYQhGOyi6L%2BVblvHMjLVBFXNr%2FfnJEM6%2FXE6gIVfM2u9948bzTbBcYLZ552LzXJdBpXqFNQ8t8D4VOYrGNXJOvxCnOI5OlmORzEvHNS1USQhq0rNb1JY8X1N6oVvcIGkrlOJfcJgWT6oTnI0L5CBtzbVVtvwsjvomeb3ZlmbWXCohNxkSCBJouc7zXTRXaejPr8dUBHpxAvgXD6Qch%2Fnm19OoaspXOpxX2oXV5z8wKNibmYgGMiqG2Tnp1d5ZY7Vai14pdD2OijlQWmQsE0FYfpcqYAyL9xbO6Iv3kB5v7MY%3D&X-Amz-Signature=393509c6b2e96db5d0f10557b5973b7bd167a2f7349910073d1c8ddd72fd8992&X-Amz-SignedHeaders=host\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3Fyi_8bnm1JP",
   "metadata": {
    "id": "3Fyi_8bnm1JP"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow-io -q\n",
    "!pip install tensorflow-addons -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f49591d",
   "metadata": {
    "id": "0f49591d"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from create_audio_tfrecords import AudioTarReader, PersonIdAudio\n",
    "\n",
    "audio_tarfile = 'data/en.tar'\n",
    "audio_tarfile = 'data/cv-corpus-7.0-2021-07-21-pt.tar.gz'\n",
    "en_total = 1584330\n",
    "sr = 48000\n",
    "\n",
    "atr = AudioTarReader(audio_tarfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26132e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "atr.data_files['train.tsv'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cf1364",
   "metadata": {
    "id": "b3cf1364"
   },
   "outputs": [],
   "source": [
    "audio_content = atr.retrieve_per_user_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4009820",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49278dcc",
   "metadata": {
    "id": "49278dcc"
   },
   "source": [
    "## Sanity check\n",
    "\n",
    "Check if audios from the same person sound like that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b21d20",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "62b21d20",
    "outputId": "42981a76-f47b-4178-faf3-b87e24aecca4"
   },
   "outputs": [],
   "source": [
    "temp_list = [x for x in audio_content if len(audio_content[x]) > 2]\n",
    "cur_idx = np.random.randint(len(temp_list))\n",
    "audio_samples = audio_content[temp_list[cur_idx]]\n",
    "len(audio_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fe3a9e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "60fe3a9e",
    "outputId": "80c1a763-0e6d-4250-9e9c-0939382403d1"
   },
   "outputs": [],
   "source": [
    "decoded_mp3 = tfio.audio.decode_mp3(audio_samples[0])\n",
    "Audio(decoded_mp3.numpy()[:, 0], rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3454c963",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "3454c963",
    "outputId": "e7e55257-aaba-4268-fab2-120b5c42721b"
   },
   "outputs": [],
   "source": [
    "decoded_mp3 = tfio.audio.decode_mp3(audio_samples[1])\n",
    "Audio(decoded_mp3.numpy()[:, 0], rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef21018",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ef21018",
    "outputId": "74ab4235-3811-40a7-fb19-7f3c5fb1705a"
   },
   "outputs": [],
   "source": [
    "len(audio_samples[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf66437",
   "metadata": {
    "id": "0bf66437"
   },
   "source": [
    "# Model training\n",
    "\n",
    "## Retrieve tf.records.dataset\n",
    "\n",
    "PersonIdAudio contains code to retrieve a tf.records.Dataset from a given audio_content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce4f400",
   "metadata": {
    "id": "cce4f400"
   },
   "outputs": [],
   "source": [
    "pia = PersonIdAudio(audio_content, sr)\n",
    "audio_dataset = pia.get_tf_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83da9075",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "83da9075",
    "outputId": "60aa6bbc-2426-4980-8de0-0d7a2101cf4e"
   },
   "outputs": [],
   "source": [
    "samples = [x for x in audio_dataset.take(10)]\n",
    "decoded_mp3 = tfio.audio.decode_mp3(samples[0][0])\n",
    "Audio(decoded_mp3.numpy()[:, 0], rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ea4531",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "72ea4531",
    "outputId": "2a75cd2a-bdef-4015-f248-5ac2fa11d6b3"
   },
   "outputs": [],
   "source": [
    "decoded_mp3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53078b1d",
   "metadata": {
    "id": "53078b1d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c68c3e50",
   "metadata": {},
   "source": [
    "## Write tfrecords file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c117b630",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfrecords_file = pia.save_tfrecords_file('pt-train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1703c4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if tfrecords file is OK\n",
    "# notice GZIP compression + the deserialization function map\n",
    "tfrecords_audio_dataset = tf.data.TFRecordDataset(\n",
    "    tfrecords_file, compression_type='GZIP'\n",
    ").map(PersonIdAudio.deserialize_from_tfrecords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6f0cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [x for x in tfrecords_audio_dataset.take(4)]\n",
    "decoded_mp3 = tfio.audio.decode_mp3(samples[1][0])\n",
    "Audio(decoded_mp3.numpy()[:, 0], rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d37da08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "998db83b",
   "metadata": {
    "id": "998db83b"
   },
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79b10be",
   "metadata": {
    "id": "f79b10be"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import layers as L\n",
    "# good example here\n",
    "# https://www.tensorflow.org/addons/tutorials/losses_triplet\n",
    "\n",
    "n_mel_bins = 80\n",
    "\n",
    "def normalized_mel_spectrogram(x, sr=48000):\n",
    "    spec_stride = 256\n",
    "    spec_len = 1024\n",
    "\n",
    "    spectrogram = tfio.audio.spectrogram(\n",
    "        x, nfft=spec_len, window=spec_len, stride=spec_stride\n",
    "    )\n",
    "\n",
    "    num_spectrogram_bins = spec_len // 2 + 1  # spectrogram.shape[-1]\n",
    "    lower_edge_hertz, upper_edge_hertz, num_mel_bins = 80.0, 10000.0, n_mel_bins\n",
    "    linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\n",
    "      num_mel_bins, num_spectrogram_bins, sr, lower_edge_hertz,\n",
    "      upper_edge_hertz)\n",
    "    mel_spectrograms = tf.tensordot(\n",
    "      spectrogram, linear_to_mel_weight_matrix, 1)\n",
    "    mel_spectrograms.set_shape(spectrogram.shape[:-1].concatenate(\n",
    "      linear_to_mel_weight_matrix.shape[-1:]))\n",
    "\n",
    "    # Compute a stabilized log to get log-magnitude mel-scale spectrograms.\n",
    "    log_mel_spectrograms = tf.math.log(mel_spectrograms + 1e-6)\n",
    "    avg = tf.math.reduce_mean(log_mel_spectrograms)\n",
    "    std = tf.math.reduce_std(log_mel_spectrograms)\n",
    "\n",
    "    return (log_mel_spectrograms - avg) / std\n",
    "\n",
    "\n",
    "def BaseSpeechEmbeddingModel(inputLength=None, rnn_func=L.LSTM, rnn_units=64):\n",
    "    # input is the first channel of the decoded mp3, ie, \n",
    "    # tfio.audio.decode_mp3(data)[:, 0]\n",
    "\n",
    "    # inp = L.Input((inputLength,), name='input')\n",
    "    # mel_spec = L.Lambda(lambda z: normalized_mel_spectrogram(z), name='normalized_spectrogram')(inp)\n",
    "\n",
    "    # receive normalized mel spectrogram as input instead\n",
    "    inp = L.Input((inputLength, n_mel_bins), name='input')\n",
    "    mel_spec = inp\n",
    "\n",
    "    # normalize the spectrogram\n",
    "    # mel_spec = L.BatchNormalization()(mel_spec)\n",
    "    # mel_spec = L.LayerNormalization()(mel_spec)\n",
    "\n",
    "    x = L.Bidirectional(\n",
    "        rnn_func(rnn_units, return_sequences=True)\n",
    "    )(mel_spec)  # [b_s, seq_len, vec_dim]\n",
    "    x = L.Bidirectional(\n",
    "        rnn_func(rnn_units, return_sequences=False)\n",
    "    )(x)  # [b_s, seq_len, vec_dim]\n",
    "\n",
    "    x = L.Dense(rnn_units, activation=None)(x)  # No activation on final dense layer\n",
    "    # L2 normalize embeddings\n",
    "    # note: L2 returns normalized, norm\n",
    "    x = L.Lambda(lambda z: tf.math.l2_normalize(z, axis=1), name='output')(x)\n",
    "    \n",
    "    output = x\n",
    "\n",
    "    model = Model(inputs=[inp], outputs=[output])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932e0493",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "932e0493",
    "outputId": "762a7037-18ec-443a-c854-7b2d37e675d2"
   },
   "outputs": [],
   "source": [
    "m = BaseSpeechEmbeddingModel()\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a67cd0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f3a67cd0",
    "outputId": "beaab2a3-8f8a-4749-8484-d057c889595e"
   },
   "outputs": [],
   "source": [
    "mel_spec = normalized_mel_spectrogram(decoded_mp3[:, 0])\n",
    "v = tf.expand_dims(mel_spec, axis=0)\n",
    "v = tf.concat([v, v], axis=0)\n",
    "pred = m.predict(v)\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b445562a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "id": "b445562a",
    "outputId": "3c9ed633-4cf2-45f5-aea1-5d461baa7c17"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.pcolormesh(tf.transpose(mel_spec.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf68124",
   "metadata": {
    "id": "8cf68124"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b0da76",
   "metadata": {
    "id": "71b0da76"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9249ec15",
   "metadata": {
    "id": "9249ec15"
   },
   "source": [
    "TODO:\n",
    "\n",
    "- Train only with the mel-spectrogram\n",
    "- Make the tfrecords file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889ae7cf",
   "metadata": {
    "id": "889ae7cf"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb1e213",
   "metadata": {
    "id": "ecb1e213"
   },
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b95296",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "id": "39b95296",
    "outputId": "51438110-168d-461a-b602-cfb7f9bd5a71"
   },
   "outputs": [],
   "source": [
    "batch_size = 48\n",
    "\n",
    "return_mel_spec = True\n",
    "def mp3_decode_fn(audio_bytes, audio_class):\n",
    "    # check if limiting output size helps\n",
    "    # return tfio.audio.decode_mp3(audio_bytes)[:, 0], audio_class\n",
    "    audio_data = tfio.audio.decode_mp3(audio_bytes)[:, 0]\n",
    "    # audio_data = tfio.audio.decode_mp3(audio_bytes)[0:48000 * 4, 0]\n",
    "    if return_mel_spec:\n",
    "        audio_data = normalized_mel_spectrogram(audio_data)\n",
    "    return audio_data, audio_class\n",
    "\n",
    "train_set = audio_dataset.map(  # Reduce memory usage\n",
    "        mp3_decode_fn,\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    ).cache().repeat(\n",
    "    ).shuffle(\n",
    "        10 * batch_size,\n",
    "        reshuffle_each_iteration=True\n",
    "    ).padded_batch(  # Vectorize your mapped function\n",
    "        batch_size,  # batch size\n",
    "        drop_remainder=True\n",
    "    ).prefetch(  # Overlap producer and consumer works\n",
    "        tf.data.AUTOTUNE\n",
    "    )\n",
    "\n",
    "\"\"\"\n",
    "train_set = audio_dataset.cache(filename='data/audio_data.cache').repeat(\n",
    "    ).map(  # Reduce memory usage\n",
    "        mp3_decode_fn,\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    ).padded_batch(  # Vectorize your mapped function\n",
    "        batch_size,  # batch size\n",
    "        drop_remainder=True\n",
    "    ).shuffle(\n",
    "        10 * batch_size,\n",
    "        reshuffle_each_iteration=True\n",
    "    ).prefetch(  # Overlap producer and consumer works\n",
    "        tf.data.AUTOTUNE\n",
    "    )\n",
    "    \n",
    "    # .interleave(  # Parallelize data reading\n",
    "    #     dataset_generator_fun,\n",
    "    #     num_parallel_calls=tf.data.AUTOTUNE\n",
    "    # )\n",
    "    # .map(  # Parallelize map transformation\n",
    "    #     time_consuming_map,\n",
    "    #     num_parallel_calls=tf.data.AUTOTUNE\n",
    "    # )\n",
    "    .cache()  # Cache data\n",
    "    .map(  # Reduce memory usage\n",
    "        mp3_decode_fn,\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    .padded_batch(  # Vectorize your mapped function\n",
    "        256,  # batch size\n",
    "        drop_remainder=True\n",
    "    )\n",
    "    .shuffle(\n",
    "        5000,\n",
    "        reshuffle_each_iteration=True\n",
    "    )\n",
    "    .prefetch(  # Overlap producer and consumer works\n",
    "        tf.data.AUTOTUNE\n",
    "    )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba231d7f",
   "metadata": {
    "id": "ba231d7f"
   },
   "outputs": [],
   "source": [
    "# sample_train_data = [x for x in train_set.take(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83b19a0",
   "metadata": {
    "id": "f83b19a0"
   },
   "outputs": [],
   "source": [
    "# elements, contents/labels\n",
    "# len(sample_train_data), sample_train_data[0][0].shape, sample_train_data[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a253d1",
   "metadata": {
    "id": "c7a253d1"
   },
   "outputs": [],
   "source": [
    "# cur_sample = sample_train_data[0][0][5].numpy()\n",
    "# Audio(cur_sample, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a643121",
   "metadata": {
    "id": "6a643121"
   },
   "outputs": [],
   "source": [
    "# m.predict(sample_train_data[0][0][0:8]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2090b2c7",
   "metadata": {
    "id": "2090b2c7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc07a03",
   "metadata": {
    "id": "6bc07a03"
   },
   "outputs": [],
   "source": [
    "m.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    loss=tfa.losses.TripletSemiHardLoss()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb78c9b5",
   "metadata": {
    "id": "cb78c9b5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ecb3c6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a8ecb3c6",
    "outputId": "193e7593-0535-492e-bcb0-3a84e7980fd4"
   },
   "outputs": [],
   "source": [
    "history = m.fit(\n",
    "    train_set,\n",
    "    steps_per_epoch = pia.n_audios // batch_size,\n",
    "    epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3681d6b2",
   "metadata": {
    "id": "3681d6b2"
   },
   "outputs": [],
   "source": [
    "m.save(f\"model_{audio_tarfile.replace('/', '').replace('.', '')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6763655",
   "metadata": {
    "id": "d6763655"
   },
   "outputs": [],
   "source": [
    "m.save(f\"model_{audio_tarfile.replace('/', '').replace('.', '')}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yHPbJvA3-cz7",
   "metadata": {
    "id": "yHPbJvA3-cz7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ttFcHUWa-dX9",
   "metadata": {
    "id": "ttFcHUWa-dX9"
   },
   "source": [
    "### Check what the similarities look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-NU_SYEqNFhy",
   "metadata": {
    "id": "-NU_SYEqNFhy"
   },
   "outputs": [],
   "source": [
    "val_path_to_client_dict = dict(zip(data_files['dev.tsv'].path, data_files['dev.tsv'].client_id))\n",
    "val_audio_content = {}\n",
    "for x in tqdm(tar_file_list):\n",
    "    name_split = x.name.split('/')\n",
    "    cur_id = val_path_to_client_dict.get(name_split[-1], False)\n",
    "    if cur_id:\n",
    "        audio_data = audios_tar.extractfile(x).read()\n",
    "        cur_id_dict = val_audio_content.get(cur_id, [])\n",
    "        cur_id_dict.append(audio_data)\n",
    "        val_audio_content[cur_id] = cur_id_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Q910FI8g-c2Y",
   "metadata": {
    "id": "Q910FI8g-c2Y"
   },
   "outputs": [],
   "source": [
    "def get_embedding(data, model):\n",
    "    preds = []\n",
    "    for x in tqdm(data):\n",
    "        audio_data = tfio.audio.decode_mp3(x)[:, 0]\n",
    "        audio_data = normalized_mel_spectrogram(audio_data)\n",
    "        cur_pred = model.predict(\n",
    "            tf.expand_dims(audio_data, axis=0)\n",
    "        )[0]\n",
    "        preds.append(cur_pred)\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ry8AVSZo-c6f",
   "metadata": {
    "id": "ry8AVSZo-c6f"
   },
   "outputs": [],
   "source": [
    "audio_content_with_repeats = [x for x in val_audio_content if len(val_audio_content[x]) > 1]\n",
    "print([len(val_audio_content[x]) for x in audio_content_with_repeats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8nUE5ylZLZyD",
   "metadata": {
    "id": "8nUE5ylZLZyD"
   },
   "outputs": [],
   "source": [
    "len(val_audio_content[audio_content_with_repeats[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3333704",
   "metadata": {
    "id": "f3333704"
   },
   "outputs": [],
   "source": [
    "all_keys = audio_content_with_repeats\n",
    "samples1 = val_audio_content[all_keys[4]]\n",
    "samples2 = val_audio_content[all_keys[18]]\n",
    "preds1 = get_embedding(samples1, m)\n",
    "preds2 = get_embedding(samples2, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CUKA5eKrA_JV",
   "metadata": {
    "id": "CUKA5eKrA_JV"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_dists(list1, list2):\n",
    "    ans = []\n",
    "    for x in tqdm(list1):\n",
    "        for y in list2:\n",
    "            dist = np.linalg.norm(x-y)\n",
    "            ans.append(dist)\n",
    "    return ans\n",
    "\n",
    "local_dists1 = get_dists(preds1, preds1)\n",
    "local_dists2 = get_dists(preds2, preds2)\n",
    "cross_dists = get_dists(preds1, preds2)\n",
    "\n",
    "np.mean(local_dists1), np.mean(local_dists2), np.mean(cross_dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jnUdx9ozA_Lt",
   "metadata": {
    "id": "jnUdx9ozA_Lt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WU3-HQnkA_OI",
   "metadata": {
    "id": "WU3-HQnkA_OI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec8774d4",
   "metadata": {
    "id": "ec8774d4"
   },
   "source": [
    "### Debug code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d072bf56",
   "metadata": {
    "id": "d072bf56"
   },
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0a9597",
   "metadata": {
    "id": "ee0a9597"
   },
   "outputs": [],
   "source": [
    "plt.plot(decoded_mp3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df02c252",
   "metadata": {
    "id": "df02c252"
   },
   "outputs": [],
   "source": [
    "mel_spec = m.predict(tf.expand_dims(decoded_mp3[:, 0], axis=0))\n",
    "mel_spec = tf.transpose(mel_spec[0])\n",
    "plt.pcolormesh(mel_spec.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c9d0bd",
   "metadata": {
    "id": "b0c9d0bd"
   },
   "outputs": [],
   "source": [
    "tf.math.reduce_mean(mel_spec), tf.math.reduce_max(mel_spec), tf.math.reduce_min(mel_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b9cfab",
   "metadata": {
    "id": "37b9cfab"
   },
   "outputs": [],
   "source": [
    "plt.hist(mel_spec.numpy().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ec49bb",
   "metadata": {
    "id": "88ec49bb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf3d518",
   "metadata": {
    "id": "8cf3d518"
   },
   "outputs": [],
   "source": [
    "# figure out the mp3 folder inside tar\n",
    "\n",
    "for x in audios_tar:\n",
    "    if x.name.endswith('.mp3'):\n",
    "        mp3_folder = x.name.split('/')\n",
    "        mp3_folder = '/'.join(mp3_folder[:-1])\n",
    "        break\n",
    "print(f'Detected mp3 folder: {mp3_folder}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8275760",
   "metadata": {
    "id": "b8275760"
   },
   "outputs": [],
   "source": [
    "# build a dictionary with key -> person id, value -> list of audios from that person\n",
    "\n",
    "audio_per_person = {}\n",
    "audio_list = data_files['train.tsv'][['client_id', 'path']].values[0:10]\n",
    "\n",
    "for (person_id, audio_file) in tqdm(audio_list):\n",
    "    audio_data = audios_tar.extractfile(mp3_folder + '/' + audio_file).read()\n",
    "    person_audio_list = audio_per_person.get(person_id, [])\n",
    "    person_audio_list.append(audio_data)\n",
    "    audio_per_person[person_id] = person_audio_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf5f475",
   "metadata": {
    "id": "eaf5f475"
   },
   "outputs": [],
   "source": [
    "# check a few audios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f32216",
   "metadata": {
    "id": "34f32216"
   },
   "outputs": [],
   "source": [
    "data_files['train.tsv'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac472fb6",
   "metadata": {
    "id": "ac472fb6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Train voice embedding model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "195e3caf63d148bf84b91d08eca43441": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": " 14%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d9663a41a0a044a9bfd31c9c37a7b909",
      "max": 1584330,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4e00b0138afa4aed96a1fdbdab3a093e",
      "value": 213936
     }
    },
    "1e0c9f8a66e14d8cbac0f35a80c79f45": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2d3d9635b4354eafa308fd19137bc601": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7d74f06ec4784026b7acbf8a34ff5909",
      "placeholder": "​",
      "style": "IPY_MODEL_2dfdc89f8d2b47d2997a7e123a119ca3",
      "value": " 213936/1584330 [07:31&lt;48:11, 474.00it/s]"
     }
    },
    "2dfdc89f8d2b47d2997a7e123a119ca3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "39ee492c912c407e8091eb7625be1a1a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_828f3584d0164d75a8642876dcf03a1a",
      "max": 213936,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fce1d3674ccd46f6b5af946f8d830249",
      "value": 213936
     }
    },
    "3caeb70669bd4ffc810c82b7d099f8f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_195e3caf63d148bf84b91d08eca43441",
       "IPY_MODEL_2d3d9635b4354eafa308fd19137bc601"
      ],
      "layout": "IPY_MODEL_1e0c9f8a66e14d8cbac0f35a80c79f45"
     }
    },
    "45c69ab018f24f7799ae67627117c34c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4e00b0138afa4aed96a1fdbdab3a093e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "7d74f06ec4784026b7acbf8a34ff5909": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "828f3584d0164d75a8642876dcf03a1a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8653fcda002645c2a9eb35ba410c90d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a1d5577f0ba24665a9948ae3d8f3c41a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_39ee492c912c407e8091eb7625be1a1a",
       "IPY_MODEL_c9bd8d9a7b8f4428b5cf5e3fce81d988"
      ],
      "layout": "IPY_MODEL_45c69ab018f24f7799ae67627117c34c"
     }
    },
    "c9bd8d9a7b8f4428b5cf5e3fce81d988": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e80b26847ee7482f8e5df91645e39e2f",
      "placeholder": "​",
      "style": "IPY_MODEL_8653fcda002645c2a9eb35ba410c90d8",
      "value": " 213936/213936 [03:46&lt;00:00, 945.09it/s]"
     }
    },
    "d9663a41a0a044a9bfd31c9c37a7b909": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e80b26847ee7482f8e5df91645e39e2f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fce1d3674ccd46f6b5af946f8d830249": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
