{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9c4beb2",
   "metadata": {},
   "source": [
    "# Speech Identity Inference\n",
    "\n",
    "Let's check if the pretrained model can really identify speakers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49845bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import Audio\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from train_speech_id_model import BaseSpeechEmbeddingModel\n",
    "from create_audio_tfrecords import AudioTarReader, PersonIdAudio\n",
    "\n",
    "sr = 48000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b27d84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = BaseSpeechEmbeddingModel()\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6690aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.load_weights('temp/cp-0060.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdc720b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the corpus to other languages allows evaluating how the model transfers between languages\n",
    "dev_dataset = tfrecords_audio_dataset = tf.data.TFRecordDataset(\n",
    "    'data/cv-corpus-7.0-2021-07-21-pt.tar.gz_dev.tfrecords.gzip', compression_type='GZIP',\n",
    "    num_parallel_reads=4\n",
    ").map(PersonIdAudio.deserialize_from_tfrecords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bfa421",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [x for x in dev_dataset.take(1000)]\n",
    "# decode audio\n",
    "samples = [(tfio.audio.decode_mp3(x[0])[:, 0], x[1]) for x in samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf84c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# is the audio decoded correctly?\n",
    "Audio(samples[10][0], rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e643b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the embeddings\n",
    "embeddings = []\n",
    "for audio_data, person_id in tqdm(samples):\n",
    "    cur_emb = m.predict(\n",
    "        tf.expand_dims(audio_data, axis=0)\n",
    "    )\n",
    "    embeddings.append(cur_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ae7f58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c12f963",
   "metadata": {},
   "source": [
    "## Check embedding quality\n",
    "\n",
    "Ideally, embeddings from the same person should look the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724c2748",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c404034",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_speakers = len(set([x[1].numpy() for x in samples]))\n",
    "print(f'Loaded {n_speakers} different speakers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faf722c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_diff = {'same': [], 'different': []}\n",
    "for p in tqdm(range(len(samples))):\n",
    "    for q in range(p + 1, len(samples)):\n",
    "        id_1 = samples[p][1]\n",
    "        id_2 = samples[q][1]\n",
    "        dist = np.linalg.norm(embeddings[p] - embeddings[q])\n",
    "        if id_1 == id_2:\n",
    "            pairwise_diff['same'].append(dist)\n",
    "        else:\n",
    "            pairwise_diff['different'].append(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643b3662",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot([pairwise_diff[x] for x in pairwise_diff])\n",
    "plt.xticks([k + 1 for k in range(len(pairwise_diff))], [x for x in pairwise_diff])\n",
    "plt.ylabel('Embedding distance')\n",
    "plt.title('Boxplot of speaker identifiability')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2d3ad7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b531c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what do we care about?\n",
    "# given that 2 samples are different, we don't want to predict `same`\n",
    "# secondarily, given that 2 samples are the same, we want to predict `same`\n",
    "\n",
    "# threshold\n",
    "t = 0.2 * np.median(pairwise_diff['different']) + np.median(pairwise_diff['same']) * 0.8\n",
    "\n",
    "specificity = np.sum(np.array(pairwise_diff['different'] > t)) / len(pairwise_diff['different'])\n",
    "sensitivity = np.sum(np.array(pairwise_diff['same'] < t)) / len(pairwise_diff['same'])\n",
    "\n",
    "print('Sensitivity, specificity = ', sensitivity, specificity)\n",
    "\n",
    "same_lbl = [0] * len(pairwise_diff['same'])\n",
    "diff_lbl = [1] * len(pairwise_diff['different'])\n",
    "scores = pairwise_diff['same'] + pairwise_diff['different']\n",
    "\n",
    "scores = np.array(scores) * 0.5\n",
    "t = t * 0.5\n",
    "\n",
    "labels = same_lbl + diff_lbl\n",
    "len(scores), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6e4321",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "fpr, tpr, thresholds = metrics.roc_curve(labels, scores, pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965fbb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "\n",
    "plt.plot(fpr, tpr)\n",
    "plt.plot([0, 1], [0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcad0740",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Point of operation')\n",
    "plt.plot(thresholds, 1 - fpr, label='Specificity')\n",
    "plt.plot(thresholds, tpr, label='Sensitivity')\n",
    "plt.plot([t, t], [0, 1], label='Threshold')\n",
    "plt.xlabel('Threshold level')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6d2866",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd36fe91",
   "metadata": {},
   "source": [
    "## Select best model on validation\n",
    "\n",
    "Strategy: compute loss but don't sort validation set, so there are multiple voice repeats in a batch. Also makes the evaluation consistent. Batch size should be as big as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4512952d",
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_loss = tfa.losses.TripletSemiHardLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27164a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute all predictions\n",
    "def mp3_decode_fn(audio_bytes, audio_class):\n",
    "    audio_data = tfio.audio.decode_mp3(audio_bytes)[:, 0]\n",
    "    return audio_data, audio_class\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "for x in tqdm(dev_dataset.take(1300).map(mp3_decode_fn)):\n",
    "    s = x[0]\n",
    "    all_preds.append(m.predict(\n",
    "        tf.expand_dims(x[0], axis=0)\n",
    "    )[0])\n",
    "    all_labels.append(x[1].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1346ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3266f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "n_batches = len(all_preds) // batch_size\n",
    "vec_size = len(all_preds[0])\n",
    "\n",
    "np_preds = np.reshape(all_preds[0:batch_size * n_batches], (n_batches, batch_size, vec_size))\n",
    "np_labls = np.reshape(all_labels[0:batch_size * n_batches], (n_batches, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da175f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss = 0\n",
    "for lbl, pred in zip(np_labls, np_preds):\n",
    "    total_loss += triplet_loss(lbl, pred).numpy()\n",
    "total_loss = total_loss / len(lbl)\n",
    "print(f'Total loss: {total_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e04d203",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e5157e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
