{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4efb0cbb",
   "metadata": {},
   "source": [
    "# Speech Identity Inference\n",
    "\n",
    "Let's check if the pretrained model can really identify speakers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52db0bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import Audio\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from train_speech_id_model import BaseSpeechEmbeddingModel\n",
    "from create_audio_tfrecords import AudioTarReader, PersonIdAudio\n",
    "\n",
    "sr = 48000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977abfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = BaseSpeechEmbeddingModel()\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf706fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 90.cpkt: auc = 0.9525\n",
    "# 110.cpkt: auc = 0.9533\n",
    "chkpt = 'temp/cp-0110.ckpt'\n",
    "m.load_weights(chkpt)\n",
    "m.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(0.0006),\n",
    "    loss=tfa.losses.TripletSemiHardLoss()\n",
    ")\n",
    "# m.save('speech-id-model-110')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ecb9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the corpus to other languages allows evaluating how the model transfers between languages\n",
    "dev_dataset = tfrecords_audio_dataset = tf.data.TFRecordDataset(\n",
    "    'data/cv-corpus-7.0-2021-07-21-en.tar.gz_dev.tfrecords.gzip', compression_type='GZIP',\n",
    "#    'data/cv-corpus-7.0-2021-07-21-en.tar.gz_test.tfrecords.gzip', compression_type='GZIP',\n",
    "    num_parallel_reads=4\n",
    ").map(PersonIdAudio.deserialize_from_tfrecords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c61b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [x for x in dev_dataset.take(2500)]\n",
    "# decode audio\n",
    "samples = [(tfio.audio.decode_mp3(x[0])[:, 0], x[1]) for x in samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6249562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# is the audio decoded correctly?\n",
    "Audio(samples[10][0], rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba43ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the embeddings\n",
    "embeddings = []\n",
    "for audio_data, person_id in tqdm(samples):\n",
    "    cur_emb = m.predict(\n",
    "        tf.expand_dims(audio_data, axis=0)\n",
    "    )[0]\n",
    "    embeddings.append(cur_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f811bd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3988be4",
   "metadata": {},
   "source": [
    "## Check embedding quality\n",
    "\n",
    "Ideally, embeddings from the same person should look the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b584168",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba7cf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_speakers = len(set([x[1].numpy() for x in samples]))\n",
    "print(f'Loaded {n_speakers} different speakers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e83509",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_diff = {'same': [], 'different': []}\n",
    "for p in tqdm(range(len(samples))):\n",
    "    for q in range(p + 1, len(samples)):\n",
    "        id_1 = samples[p][1]\n",
    "        id_2 = samples[q][1]\n",
    "        dist = np.linalg.norm(embeddings[p] - embeddings[q])\n",
    "        if id_1 == id_2:\n",
    "            pairwise_diff['same'].append(dist)\n",
    "        else:\n",
    "            pairwise_diff['different'].append(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e2169d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.boxplot([pairwise_diff[x] for x in pairwise_diff])\n",
    "plt.xticks([k + 1 for k in range(len(pairwise_diff))], [x for x in pairwise_diff])\n",
    "plt.ylabel('Embedding distance')\n",
    "plt.title('Boxplot of speaker identifiability')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82d0542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what do we care about?\n",
    "# given that 2 samples are different, we don't want to predict `same`\n",
    "# secondarily, given that 2 samples are the same, we want to predict `same`\n",
    "\n",
    "# threshold - alpha from 0 (median of same) to 1 (median of different)\n",
    "alpha = 0.2\n",
    "\n",
    "# if using the validation set, we can calibrate t\n",
    "t = np.median(pairwise_diff['same']) + alpha * (np.median(pairwise_diff['different']) - np.median(pairwise_diff['same']))\n",
    "\n",
    "specificity = np.sum(np.array(pairwise_diff['different']) > t) / len(pairwise_diff['different'])\n",
    "sensitivity = np.sum(np.array(pairwise_diff['same']) < t) / len(pairwise_diff['same'])\n",
    "\n",
    "print('Sensitivity, specificity = ', sensitivity, specificity)\n",
    "\n",
    "same_lbl = [0] * len(pairwise_diff['same'])\n",
    "diff_lbl = [1] * len(pairwise_diff['different'])\n",
    "scores = pairwise_diff['same'] + pairwise_diff['different']\n",
    "\n",
    "# scale scores to range [0,1] and chande threshold accordingly\n",
    "scores = np.array(scores) * 0.5\n",
    "t = t * 0.5\n",
    "\n",
    "labels = same_lbl + diff_lbl\n",
    "len(scores), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc01812",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(labels, scores, pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d62920",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "roc_auc = metrics.roc_auc_score(labels, scores)\n",
    "\n",
    "plt.title(f'ROC curve: AUC = {np.round(roc_auc, 4)} {chkpt}')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "\n",
    "plt.plot(fpr, tpr)\n",
    "plt.plot([0, 1], [0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7eadfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.title('Point of operation')\n",
    "plt.plot(thresholds, 1 - fpr, label='Specificity')\n",
    "plt.plot(thresholds, tpr, label='Sensitivity')\n",
    "plt.plot([t, t], [0, 1], label='Threshold')\n",
    "plt.xlabel('Threshold level')\n",
    "plt.xlim([0, 1])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b43bcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "febb9222",
   "metadata": {},
   "source": [
    "## Select best model on validation\n",
    "\n",
    "Strategy: compute loss but don't sort validation set, so there are multiple voice repeats in a batch. Also makes the evaluation consistent. Batch size should be as big as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95fd031",
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_loss = tfa.losses.TripletSemiHardLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9810b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute all predictions\n",
    "def mp3_decode_fn(audio_bytes, audio_class):\n",
    "    audio_data = tfio.audio.decode_mp3(audio_bytes)[:, 0]\n",
    "    return audio_data, audio_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedf101b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = []\n",
    "all_labels = []\n",
    "for x in tqdm(dev_dataset.take(1300).map(mp3_decode_fn)):\n",
    "    s = x[0]\n",
    "    all_preds.append(m.predict(\n",
    "        tf.expand_dims(x[0], axis=0)\n",
    "    )[0])\n",
    "    all_labels.append(x[1].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e33f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0451dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "n_batches = len(all_preds) // batch_size\n",
    "vec_size = len(all_preds[0])\n",
    "\n",
    "np_preds = np.reshape(all_preds[0:batch_size * n_batches], (n_batches, batch_size, vec_size))\n",
    "np_labls = np.reshape(all_labels[0:batch_size * n_batches], (n_batches, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e4dfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss = 0\n",
    "for lbl, pred in zip(np_labls, np_preds):\n",
    "    total_loss += triplet_loss(lbl, pred).numpy()\n",
    "total_loss = total_loss / len(lbl)\n",
    "print(f'Total loss: {total_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca4b653",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01ba806",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f78b253",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_checkpoints = [x.split('.')[0] + '.ckpt' for x in os.listdir('temp') if 'ckpt.index' in x]\n",
    "all_results = []\n",
    "for checkpoint in tqdm(all_checkpoints):\n",
    "    m.load_weights(os.path.join('temp', checkpoint))\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    n_items = 4600\n",
    "    for x in tqdm(dev_dataset.take(n_items).map(mp3_decode_fn),\n",
    "                 total=n_items, leave=False):\n",
    "    # for x in tqdm(dev_dataset.map(mp3_decode_fn),\n",
    "    #               leave=False):\n",
    "        s = x[0]\n",
    "        all_preds.append(m.predict(\n",
    "            tf.expand_dims(x[0], axis=0)\n",
    "        )[0])\n",
    "        all_labels.append(x[1].numpy())\n",
    "\n",
    "    batch_size = 128\n",
    "    n_batches = len(all_preds) // batch_size\n",
    "    vec_size = len(all_preds[0])\n",
    "\n",
    "    np_preds = np.reshape(all_preds[0:batch_size * n_batches], (n_batches, batch_size, vec_size))\n",
    "    np_labls = np.reshape(all_labels[0:batch_size * n_batches], (n_batches, batch_size))\n",
    "\n",
    "    total_loss = 0\n",
    "    for lbl, pred in zip(np_labls, np_preds):\n",
    "        total_loss += triplet_loss(lbl, pred).numpy()\n",
    "    total_loss = total_loss / len(lbl)\n",
    "    cur_result = {\n",
    "        'checkpoint': checkpoint,\n",
    "        'val_loss': total_loss\n",
    "    }\n",
    "    print(cur_result)\n",
    "    all_results.append(cur_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45d35af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = pd.DataFrame(all_results)\n",
    "df_val['idx'] = df_val.checkpoint.apply(lambda z: int(z.split('.')[0].split('-')[1]))\n",
    "df_val = df_val.set_index('idx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51639027",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val.to_csv('val_triplet_loss.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85722429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85af7a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
